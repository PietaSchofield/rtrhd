---
title: "R Tools for Routine Healthcare Data"
author: "Pieta"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{R Tools for Routine Healthcare Data}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

# Motivation

I currently have several CPRD project using Aurum, I will also have a GOLD project soon. There are some
common steps that need to be achieved with these projects. 

The idea is that if I can import the data into an [DuckDB](duckdb) database it will permit the use of 
SQL to join and extract data for the process of linkage and cleaning. This also means the original text
files need only be visited once. Some projects have billions of observation records so managing them in
memory in R is not really feasible. DuckDB also has [a well documented R api](duckrapi) and some very
nifty SQL dialect aspects that are worth discovering. Its biggest frustration for me at the moment is
that it seems to have a 1024 character limit on SQL statements and many of my SQL statements are more
complex that that permits. So I have to do some queries in steps creating temporary tables rather than
one big query. I am not sure if investing the time to learn [dbplyr](dplyr) might be a neat tidyverse
solution to this, but I have been using SQL for almost 40 years and perhaps I getting long in the tooth
to learn quite so many new tricks.

__NB. DuckDB can take a long time to install, a longer time than you think is reasonable
but I have never had it crash on installing so be patient__

## Caveat

This is work in progress and is an amalgamation of functions I have written over the years and found
useful but exist in many disparate packages. This is my attempt to put the really useful ones in one
place. There are still some to import, namely the HES APC data ONS data and IMD data import. There are
also other functions to be written yet. 

There are a suite of matching GOLD data processing functions that as yet have not been imported

## Install

It is available at GitHub [https://github.com/PietaSchofield/rtrhd](rtrhdgh)

```{r}
if(!require("remotes")){
  install.packages("remotes")
}
if(!require("tidyverse")){
  install.packages("tidyverse")
}
if(!require("ggplot2")){
  install.packages("ggplot2")
}
if(!require("rtrhd")){
  remotes::install_github("PietaSchofield/rtrhd")
}
```

## Setup

I have access to the CPRD Aurum Synthetic Dataset which I will I can share with folk at Liverpool
University as we have Multi-Study License. But I cannot distribute with this package. It is what I use in
this demo. Unfortunately it only has single files for each table unlike the usual many many observation
and drug_issue files. So it doesn't totally test the functionality of the table load functions.

### Paths

<div style='text-align: right'>**R code**</div>

### Make the DuckDb

I will use the duckdb package because it is reasonalbly fast for some analytics processing. It is
slightly better tuned to this task than the current itteration of SQLite and I am not going to write this
to be flexible enough to use a generic DBI SQL connection. I will leave that as a task for the dedicated
and just make the source code fully available

<div style='text-align: right'>**R code**</div>

```{r}
dbName <- "Aurum_Sythetic_Data.duckdb"
dbPath <- file.path(Sys.getenv("HOME"),"Projects","rtrhd")
patient_data <- file.path(dbPath,"Aurum_Synthetic_Data","patient_data")
lookup_data <- file.path(dbPath,"Aurum_Synthetic_Data","lookups")
  
synthdataDb <- rtrhd::make_aurum_database(dbName,dbPath,
                 patient_path=file.path(data_path,"patient_data"),
                 lookup_path=file.path(data_path,"lookups"))
```

This function wraps a bunch of functions that load the individual tables

<div style='text-align: right'>**R code**</div>

```{r}
tables <- rtrhd::list_tables(synthdataDb) 
names(tables) <- tables 
records <- lapply(tables, function(tn){
    flds <- paste0(rtrhd::list_fields(dbf=synthdataDb,tab=tn),collapse=", ")
    recs <- rtrhd::get_table(dbf=synthdataDb,
              sqlstr=paste0("SELECT COUNT(*) AS records FROM ",tn,";")) %>% pull(records)
    return(tibble(table=tn,fields=flds,records=recs))}) %>% bind_rows()
records %>% kableExtra::kable()
```

It is now possible to run SQL against this database

<div style='text-align: right'>**R code**</div>

```{r}
## START SQL GEN BLOCK
unnamed_sql <- str_c("SELECT DISTINCT
  p.patid,
  COUNT(o.medcodeid) AS observations
FROM
  patients AS p
INNER JOIN
  observations AS o
  ON p.patid=o.patid
GROUP BY
  p.patid")
tmp_sqlfile <- rtrhd::link_sql(unnamed_sql)
```

<div style='text-align: right'>**SQL code**</div>

```{sql, eval=F, file=tmp_sqlfile}
```

<div style='text-align: right'>**R code**</div>

```{r fig.width=10,fig.height=6}
rtrhd::unlink_sql(tmp_sqlfile)
res <- rtrhd::get_table(dbf=synthdataDb,sqlstr=unnamed_sql)
res %>% ggplot(aes(x=observations)) + geom_histogram(binwidth=10,fill="blue",colour="black",alpha=0.7) +
  labs(title="Distribution of Observation Counts Per Patient",
       x="Number of Observations",
       y="Frequency") + theme_minimal()
## END SQL BLOCK
```

## Other Stuff

There are several functions for importing SNOMED CT and DMplusD data available from TRUD. However the
current manefestation of the DMplusD import uses R to import the xml files and this is very very slow
unless your computer is powerful and a has a lot of memory. It still takes significant time on my 20
core 64GB RAM machine. I have found a quicker way to do this using calls to python scripts but this is
not fully implemented yet. 

Talking of slow. Duckdb takes a long time to install first time round and when it update it can also be
slow but I think it is worth it.

## Finally 

This has been developed on linux and not install and tested on windows or mac so there is not guarantee
it will work. It is actively being developed and while I will probably try to not change functionality it
is definitely a moving target and as such I haven't yet started to version control it. It is currently
morphing on a bleeding edge model. If anyone else does actually start using it I will make the effort to
change to a release model. But as I am the only user at the moment it hasn't seemed necessary.

I also haven't done clean install to check that I have all the dependencies in the Imports list so there
maybe some I haven't caught yet.

```{r buildit, eval=F}
homeDir <- Sys.getenv("HOME")
packagepath <- file.path(homeDir,"GitLab","rtrhd")
inputpath <- file.path(packagepath,"vignettes")
inputFile <- file.path(inputpath,"rtrhd_demo.Rmd")
noteDir <- file.path("/srv","http","uol","rtrhd")

fdisp <- rmarkdown::render(inputFile,encoding=encoding,output_dir=noteDir,clean=T)


rmarkdown::render(inputFile, output_format = "github_document", output_file = "README.md",
                  output_dir = packagepath)
browseURL(fdisp)
```

[rtrhdgh]: https://github.com/PietaSchofield/rtrhd
[duckdb]:  https://github.com/duckdb/duckdb
[duckrapi]: https://duckdb.org/docs/api/r
[dbplyr]: https://dbplyr.tidyverse.org/

